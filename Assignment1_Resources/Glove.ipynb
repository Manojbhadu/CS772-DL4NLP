{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec_vectors_emb = gensim.downloader.load('word2vec-google-news-300')\n",
    "# fasttext_vectors_emb = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
    "glove_vectors_emb = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        This book was very informative, covering all a...\n",
       "1        I am already a baseball fan and knew a bit abo...\n",
       "2        I didn't like this product it smudged all unde...\n",
       "3        I simply love the product. I appreciate print ...\n",
       "4        It goes on very easily and makes my eyes look ...\n",
       "                               ...                        \n",
       "49995                         it does not work((((((((((((\n",
       "49996    Really worthless, loud motor with absolutely n...\n",
       "49997    Don't waste your money on this. It does nothin...\n",
       "49998    Product does not remove ear wax. No suction, j...\n",
       "49999    If you wear hearing aids these are great for r...\n",
       "Name: reviews, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(text):\n",
    "    # return the reviews after convering then to lowercase\n",
    "    lower_text = text.copy()\n",
    "    for i in range(len(text)):\n",
    "        lower_text[i] = text[i].lower()\n",
    "    return lower_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    cleanedText = []\n",
    "    for test_str in text:\n",
    "        res = re.sub(r'[^\\w\\s]', '', test_str) \n",
    "        cleanedText.append(res)\n",
    "    return cleanedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    without_stopwords_text  = text.copy()\n",
    "    for i in range(len(text)):\n",
    "        without_stopwords_text[i] = [w for w in text[i] if w not in stop_words]\n",
    "    return without_stopwords_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_tokenization(text):\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(text)\n",
    "    encoded = t.texts_to_sequences(text)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dicts(train):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train)\n",
    "    words_to_index = tokenizer.word_index\n",
    "    return words_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gloveVector():\n",
    "#     wordMapping = {}\n",
    "#     fileName = ''\n",
    "#     with open(fileName, 'r', encoding='UTF-8') as f:\n",
    "#         for line in f:\n",
    "#             w_line = line.split()\n",
    "#             curr_word = w_line[0]\n",
    "#             wordMapping[curr_word] = np.array(w_line[1:], dtype=np.float64)\n",
    "#     return (wordMapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_to_index = get_dicts(train)\n",
    "# wordMap = gloveVector()\n",
    "# vocabSize = len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def embeddingMatrix():\n",
    "#     vocab_len = len(word_to_index)\n",
    "#     embed_vector_len = wordMap['moon'].shape[0]\n",
    "#     emb_matrix = np.zeros((vocab_len, embed_vector_len))\n",
    "#     for word, index in word_to_index.items():\n",
    "#     embedding_vector = wordMap.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         emb_matrix[index, :] = embedding_vector\n",
    "#     embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, input_length=MAX_LENGTH, weights = [emb_matrix], trainable=False)\n",
    "#     return embedding_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(text):\n",
    "    a = text.copy()\n",
    "    for i in range(len(text)):\n",
    "        a[i] = nltk.word_tokenize(text[i])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_padding(data):\n",
    "  z=[]\n",
    "  for i in data:\n",
    "    k = [r for j in i for r in j]\n",
    "    result = np.zeros(100*300)\n",
    "    result[:len(k)] = np.array(k)\n",
    "    z.append(result)\n",
    "  return (np.array(z,dtype='float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    reviews = data[\"reviews\"]\n",
    "    reviews = convert_to_lower(reviews)\n",
    "    reviews = remove_punctuation(reviews)\n",
    "    reviews = tokens(reviews)\n",
    "    emb = []\n",
    "    for i in reviews:\n",
    "      tmp=[]\n",
    "      for j in i:\n",
    "        try:\n",
    "          tmp.append(glove_vectors_emb[j])\n",
    "        except KeyError:\n",
    "          tmp.append(np.zeros((300)))\n",
    "      emb.append(tmp)\n",
    "    padded = perform_padding(emb)    \n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_reviews = preprocess_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_test_reviews = preprocess_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_activation(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x/np.sum(exp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetGlove:\n",
    "\n",
    "    def __init__(self, reviews, ratings):\n",
    "\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def build_nn(self,hiddenLayers,activationHidden):\n",
    "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
    "        self.model = Sequential()\n",
    "        self.model.add(InputLayer(input_shape=(MAX_LENGTH*300,)))\n",
    "        for i in range(hiddenLayers):\n",
    "            self.model.add(Dense(64,activation=activationHidden))\n",
    "        self.model.add(Dense(5,activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=Adam(0.0001), metrics=['accuracy'])\n",
    "        self.model.summary()\n",
    "        \n",
    "    def train_nn(self,batch_size,epochs):\n",
    "        # write the training loop here; you can use either tensorflow or pytorch\n",
    "        # print validation accuracy\n",
    "        y_train = tf.keras.utils.to_categorical(self.ratings,num_classes=5)\n",
    "        self.history = self.model.fit(self.reviews, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "        \n",
    "\n",
    "    def predict(self, reviews):\n",
    "        # return a list containing all the ratings predicted by the trained model\n",
    "        y_pred = self.model.predict(reviews)\n",
    "        pred1 = []\n",
    "        for i in range(len(y_pred)):\n",
    "            pred1.append(np.argmax(y_pred[i])+1)\n",
    "        return pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rating_list = train['ratings'].to_list()\n",
    "Y = [str(i-1) for i in train_rating_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Code to find the model with best training and test set \n",
    "def bestModelNN():\n",
    "    trainAccuraciesReLU = []\n",
    "    testAccuraciesReLU = []\n",
    "    trainAccuraciesSigmoid = []\n",
    "    testAccuraciesSigmoid = []\n",
    "    \n",
    "    for i in range(0,15):\n",
    "        M = NeuralNetGlove(preprocessed_reviews,Y)\n",
    "        M.build_nn(hiddenLayers=i,activationHidden='sigmoid')\n",
    "        M.train_nn(64,15)\n",
    "        y_pred = M.predict(preprocessed_test_reviews)\n",
    "        accuracyTest = accuracy_score(test['ratings'],y_pred)\n",
    "        accuracyTrain = M.history.history['accuracy'][-1]\n",
    "        trainAccuraciesSigmoid.append(accuracyTrain)\n",
    "        testAccuraciesSigmoid.append(accuracyTest)\n",
    "        del M\n",
    "        \n",
    "    for i in range(0,15):\n",
    "        M = NeuralNetGlove(preprocessed_reviews,Y)\n",
    "        M.build_nn(hiddenLayers=i,activationHidden='relu')\n",
    "        M.train_nn(64,15)\n",
    "        y_pred = M.predict(preprocessed_test_reviews)\n",
    "        accuracyTest = accuracy_score(test['ratings'],y_pred)\n",
    "        accuracyTrain = M.history.history['accuracy'][-1]\n",
    "        trainAccuraciesReLU.append(accuracyTrain)\n",
    "        testAccuraciesReLU.append(accuracyTest)\n",
    "        del M \n",
    "        \n",
    "    print(trainAccuraciesReLU)\n",
    "    print(testAccuraciesReLU)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Develop Report for the best model and prediction for some examples\n",
    "def report(y_pred,test,Model):\n",
    "    Classification_report = classification_report(test['ratings'],y_pred,target_names=['1','2','3','4','5'])\n",
    "    cm  = confusion_matrix(test['ratings'],y_pred)\n",
    "    print(\"Classification Report : \\n\",Classification_report)\n",
    "    print(\"Heat Map :\\n\")\n",
    "    sns.heatmap(cm,cmap=\"Blues\",annot=True,fmt='.4g',xticklabels=['1','2','3','4','5'],yticklabels=['1','2','3','4','5'])\n",
    "    tried_examples = [['I like it but dont think I would buy again.'], ['Nice looking cleaner but way smaller than 2 liters. Not as advertised.'],['Total waste of money, I used all 10 of these and got 0 results from it.']] \n",
    "    # Create the pandas DataFrame \n",
    "    df = pd.DataFrame(tried_examples, columns = ['reviews']) \n",
    "    pre_tried_examples = preprocess_data(df,word_to_index)\n",
    "    z = Model.predict(pre_tried_examples)\n",
    "    print(\"Examples :\\n\",df)\n",
    "    print(\"Predicted Values: \\n\",z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModelNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
